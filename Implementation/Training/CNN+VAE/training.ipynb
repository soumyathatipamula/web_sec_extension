{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.losses import mean_squared_error\n",
    "from tensorflow.keras import layers, Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.ops as ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_samples = pd.read_csv(\"../../Dataset_prep/CNN+VAE/benign_samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xss_payloads = pd.read_csv(\"../../Dataset_prep/CNN+VAE/xss_payloads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "for_split = xss_payloads.count()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "benign_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "benign_samples.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_samples = benign_samples[:775974]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "xss_payloads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_samples.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "benign_samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "benign_samples.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "xss_payloads.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_dataset = pd.concat([benign_samples, xss_payloads], axis=0)\n",
    "cnn_dataset.reset_index(drop=True, inplace=True)\n",
    "cnn_dataset = cnn_dataset.sample(frac=1, random_state=42)  # Shuffle rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cnn_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cnn_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts, max_len=250, normalize=False):\n",
    "    processed = []\n",
    "    for text in texts:\n",
    "        encoded = [ord(c) % 256 for c in text[:max_len]]\n",
    "        encoded += [0] * (max_len - len(encoded))  # Pad with zeros\n",
    "        if normalize:\n",
    "            encoded = [x / 255.0 for x in encoded]  # Normalize for VAE\n",
    "        processed.append(encoded)\n",
    "    return np.array(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cnn_dataset[\"payload\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess CNN dataset\n",
    "\n",
    "cnn_X = preprocess(cnn_dataset[\"payload\"], max_len=100, normalize=False).reshape(-1, 100, 1)\n",
    "cnn_y = np.array(cnn_dataset[\"xss\"]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_X = preprocess(benign_samples[\"payload\"], max_len=100, normalize=True).reshape(-1, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(f\"CNN dataset shape: {cnn_X.shape}, Labels shape: {cnn_y.shape}\")\n",
    "print(f\"VAE dataset shape: {vae_X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split CNN data\n",
    "cnn_X_train, cnn_X_test, cnn_y_train, cnn_y_test = train_test_split(cnn_X, cnn_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split VAE data\n",
    "vae_X_train, vae_X_test, _, _ = train_test_split(vae_X, vae_X, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CNN input 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cnn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.DepthwiseConv1D(5, depth_multiplier=8, activation='relu', input_shape=(100, 1)),\n",
    "    tf.keras.layers.Conv1D(16, 1, activation='relu'),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "]) \n",
    "\n",
    "cnn_model.summary()\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cnn = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint_filepath_cnn = './'\n",
    "model_checkpoint_callback_cnn = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"best_CNN.keras\",\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "history_cnn = cnn_model.fit(cnn_X_train, cnn_y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping_cnn, model_checkpoint_callback_cnn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cnn_model = tf.keras.models.load_model(\"best_CNN.keras\")\n",
    "except Exception as e :\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "y_pred_probs_cnn = cnn_model.predict(cnn_X_test)\n",
    "y_pred_cnn = (y_pred_probs_cnn > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "precision_cnn = precision_score(cnn_y_test, y_pred_cnn)\n",
    "recall_cnn = recall_score(cnn_y_test, y_pred_cnn)\n",
    "f1_cnn = f1_score(cnn_y_test, y_pred_cnn)\n",
    "roc_auc_cnn = roc_auc_score(cnn_y_test, y_pred_probs_cnn)\n",
    "mcc_cnn = matthews_corrcoef(cnn_y_test, y_pred_cnn)\n",
    "\n",
    "print(\"CNN Evaluation:\")\n",
    "print(f\"Precision: {precision_cnn}\")\n",
    "print(f\"Recall: {recall_cnn}\")\n",
    "print(f\"F1-score: {f1_cnn}\")\n",
    "print(f\"ROC AUC: {roc_auc_cnn}\")\n",
    "print(f\"MCC: {mcc_cnn}\")\n",
    "print(f\"Training Accuracy: {history_cnn.history['accuracy'][-1]}\")\n",
    "print(f\"Training Loss: {history_cnn.history['loss'][-1]}\")\n",
    "print(f\"Validation Accuracy: {history_cnn.history['val_accuracy'][-1]}\")\n",
    "print(f\"Validation Precision: {history_cnn.history['val_precision'][-1]}\")\n",
    "print(f\"Validation Recall: {history_cnn.history['val_recall'][-1]}\")\n",
    "print(f\"Validation Loss: {history_cnn.history['val_loss'][-1]}\")\n",
    "print(f\"Validation Accuracy: {history_cnn.history['val_accuracy'][-1]}\")\n",
    "\n",
    "cnn_model.save('xss_depthwise_cnn.h5')\n",
    "# Convert: tensorflowjs_converter --input_format keras --quantize_float16 xss_depthwise_cnn xss_depthwise_cnn_js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, model_name):\n",
    "    \"\"\"Plots training and validation accuracy and loss.\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plot_history(history_cnn, 'CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "test_samples = [\"<p>Test</p>\", \"<script>alert(1)</script>\"]\n",
    "test_X = preprocess(test_samples, max_len=100, normalize=False).reshape(-1, 100, 1)\n",
    "predictions = cnn_model.predict(test_X)\n",
    "\n",
    "print(\"CNN Predictions:\", predictions)\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "    if prediction[0] > 0.5:\n",
    "        result = \"Malicious\"\n",
    "    else:\n",
    "        result = \"Benign\"\n",
    "    print(f\"Sample {i+1}: '{test_samples[i]}' - Prediction: {prediction[0]:.4f} - {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "#Saving for tensorflow js\n",
    "cnn_model.export('xss_depthwise_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load data\n",
    "benign_samples = pd.read_csv(\"../../Dataset_prep/CNN+VAE/benign_samples.csv\")\n",
    "xss_payloads = pd.read_csv(\"../../Dataset_prep/CNN+VAE/xss_payloads.csv\")\n",
    "\n",
    "benign_samples = benign_samples[:775974]\n",
    "benign_samples.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "cnn_dataset = pd.concat([benign_samples, xss_payloads], axis=0)\n",
    "cnn_dataset.reset_index(drop=True, inplace=True)\n",
    "cnn_dataset = cnn_dataset.sample(frac=1, random_state=42)\n",
    "\n",
    "def preprocess(texts, max_len=100): # Removed normalize. normalization is not used in the CNN\n",
    "    \"\"\"Preprocesses text for CNN, ensuring consistent input shape.\"\"\"\n",
    "    processed = []\n",
    "    for text in texts:\n",
    "        encoded = [ord(c) % 256 for c in text[:max_len]]\n",
    "        encoded += [0] * (max_len - len(encoded))\n",
    "        processed.append(encoded)\n",
    "    return np.array(processed)\n",
    "\n",
    "# Preprocess data\n",
    "cnn_X = preprocess(cnn_dataset[\"payload\"], max_len=100).reshape(-1, 100, 1)\n",
    "cnn_y = np.array(cnn_dataset[\"xss\"]).reshape(-1, 1)\n",
    "\n",
    "# Split data\n",
    "cnn_X_train, cnn_X_test, cnn_y_train, cnn_y_test = train_test_split(\n",
    "    cnn_X, cnn_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define CNN model (explicit input shape)\n",
    "cnn_model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(100, 1)), #Explicit Input layer\n",
    "    layers.DepthwiseConv1D(5, depth_multiplier=8, activation='relu'),\n",
    "    layers.Conv1D(16, 1, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"best_CNN.keras\", monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "history = cnn_model.fit(cnn_X_train, cnn_y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Load best model\n",
    "try:\n",
    "    cnn_model = tf.keras.models.load_model(\"best_CNN.keras\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_probs = cnn_model.predict(cnn_X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "precision = precision_score(cnn_y_test, y_pred)\n",
    "recall = recall_score(cnn_y_test, y_pred)\n",
    "f1 = f1_score(cnn_y_test, y_pred)\n",
    "roc_auc = roc_auc_score(cnn_y_test, y_pred_probs)\n",
    "mcc = matthews_corrcoef(cnn_y_test, y_pred)\n",
    "\n",
    "print(\"CNN Evaluation:\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "print(f\"MCC: {mcc}\")\n",
    "print(f\"Training Accuracy: {history.history['accuracy'][-1]}\")\n",
    "print(f\"Training Loss: {history.history['loss'][-1]}\")\n",
    "print(f\"Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(f\"Validation Precision: {history.history['val_precision'][-1]}\")\n",
    "print(f\"Validation Recall: {history.history['val_recall'][-1]}\")\n",
    "print(f\"Validation Loss: {history.history['val_loss'][-1]}\")\n",
    "print(f\"Validation Accuracy: {history.history['val_accuracy'][-1]}\")\n",
    "\n",
    "# Save for TensorFlow.js\n",
    "cnn_model.save('xss_depthwise_cnn.h5')\n",
    "\n",
    "# Plotting function\n",
    "def plot_history(history, model_name):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history, 'CNN')\n",
    "\n",
    "# Test predictions\n",
    "test_samples = [\"<p>Test</p>\", \"<script>alert(1)</script>\"]\n",
    "test_X = preprocess(test_samples, max_len=100).reshape(-1, 100, 1)\n",
    "predictions = cnn_model.predict(test_X)\n",
    "\n",
    "print(\"CNN Predictions:\", predictions)\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "    result = \"Malicious\" if prediction[0] > 0.5 else \"Benign\"\n",
    "    print(f\"Sample {i+1}: '{test_samples[i]}' - Prediction: {prediction[0]:.4f} - {result}\")\n",
    "\n",
    "# Convert for TensorFlow.js\n",
    "# Use the input_shape flag to ensure the json file has the correct value.\n",
    "# !tensorflowjs_converter --input_format keras --input_shape 100,1 xss_depthwise_cnn.h5 xss_depthwise_cnn_js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cnn_model.export('xss_depthwise_cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
